{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db0855d0",
   "metadata": {},
   "source": [
    "# Timescale Vector Store (PostgreSQL)\n",
    "\n",
    "This notebook shows how to use the Postgres vector store `TimescaleVector` to store and query vector embeddings.\n",
    "\n",
    "## What is Timescale Vector?\n",
    "**[Timescale Vector](https://www.timescale.com/ai) is PostgreSQL++ for AI applications.**\n",
    "\n",
    "Timescale Vector enables you to efficiently store and query billions of vector embeddings in `PostgreSQL`.\n",
    "- Enhances `pgvector` with faster and more accurate similarity search on 1B+ vectors via DiskANN inspired indexing algorithm.\n",
    "- Enables fast time-based vector search via automatic time-based partitioning and indexing.\n",
    "- Provides a familiar SQL interface for querying vector embeddings and relational data.\n",
    "\n",
    "Timescale Vector scales with you from POC to production:\n",
    "- Simplifies operations by enabling you to store relational metadata, vector embeddings, and time-series data in a single database.\n",
    "- Benefits from rock-solid PostgreSQL foundation with enterprise-grade feature liked streaming backups and replication, high-availability and row-level security.\n",
    "- Enables a worry-free experience with enterprise-grade security and compliance.\n",
    "\n",
    "## How to use Timescale Vector\n",
    "Timescale Vector is available on [Timescale](https://www.timescale.com/products), the cloud PostgreSQL platform. (There is no self-hosted version at this time.)\n",
    "\n",
    "- LlamaIndex users get a 90-day free trial for Timescale Vector.\n",
    "- To get started, [signup](https://console.cloud.timescale.com/signup) to Timescale, create a new database and follow this notebook!\n",
    "- See the [installation instructions](https://github.com/timescale/python-vector) for more details on using Timescale Vector in python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfb491c6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's import everything we'll need for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2d1c538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:14.461233Z",
     "start_time": "2023-06-08T13:06:14.451565Z"
    }
   },
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import sys\n",
    "\n",
    "# Uncomment to see debug logs\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import SimpleDirectoryReader, StorageContext\n",
    "from llama_index.indices.vector_store import VectorStoreIndex\n",
    "from llama_index.vector_stores import TimescaleVectorStore\n",
    "import textwrap\n",
    "import openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26c71b6d",
   "metadata": {},
   "source": [
    "### Setup OpenAI API Key\n",
    "To create embeddings for documents loaded into the index, let's configure your OpenAI API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67b86621",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:16.584215Z",
     "start_time": "2023-06-08T13:06:16.576508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get openAI api key by reading local .env file\n",
    "# The .env file should contain a line starting with `OPENAI_API_KEY=sk-`\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# OR set it explicitly \n",
    "#import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"<your key>\"\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "### Loading documents\n",
    "For this example, we'll use a [SimpleDirectoryReader](https://gpt-index.readthedocs.io/en/stable/examples/data_connectors/simple_directory_reader.html) to load the documents stored in the the `paul_graham_essay` directory. \n",
    "\n",
    "The `SimpleDirectoryReader` is one of LlamaIndex's most commonly used data connectors to read one or multiple files from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c154dd4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:19.009847Z",
     "start_time": "2023-06-08T13:06:19.001593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: ecd1868c-7484-4aab-b7ba-6c70e0ae27df\n"
     ]
    }
   ],
   "source": [
    "# load sample data from the data directory using a SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"../data/paul_graham\").load_data()\n",
    "print(\"Document ID:\", documents[0].doc_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bd24f0a",
   "metadata": {},
   "source": [
    "### Create a PostgreSQL database and get a Timescale service URL\n",
    "You need a service url to connect to your Timescale database instance.\n",
    "\n",
    "To connect to your cloud PostgreSQL database in Timescale, you'll need your service URI, which can be found in the cheatsheet file you downloaded after creating a new database. The URI will look something like this: `postgres://tsdbadmin:<password>@<id>.tsdb.cloud.timescale.com:<port>/tsdb?sslmode=require`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6d61e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the service url by reading local .env file\n",
    "# The .env file should contain a line starting with `TIMESCALE_SERVICE_URL=postgresql://`\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "TIMESCALE_SERVICE_URL = os.environ[\"TIMESCALE_SERVICE_URL\"]\n",
    "\n",
    "# OR set it explicitly\n",
    "# TIMESCALE_SERVICE_URL = \"postgres://tsdbadmin:<password>@<id>.tsdb.cloud.timescale.com:<port>/tsdb?sslmode=require\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0232fd1",
   "metadata": {},
   "source": [
    "### Create a VectorStore Index with the TimescaleVectorStore\n",
    "Next, to perform a similarity search, we first create a `TimescaleVector` [vector store](https://gpt-index.readthedocs.io/en/stable/core_modules/data_modules/storage/vector_stores.html) to store our vector embeddings from the essay content. TimescaleVectorStore takes a few arguments, namely the `service_url` which we loaded above, along with a `table_name` which we will be the name of the table that the vectors are stored in.\n",
    "\n",
    "Then we create a [Vector Store Index](https://gpt-index.readthedocs.io/en/stable/community/integrations/vector_stores.html#vector-store-index) on the documents backed by Timescale using the previously documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8731da62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:23.856002Z",
     "start_time": "2023-06-08T13:06:20.486349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a TimescaleVectorStore to store the documents\n",
    "vector_store = TimescaleVectorStore.from_params(\n",
    "    service_url=TIMESCALE_SERVICE_URL,\n",
    "    table_name=\"paul_graham_essay\",\n",
    ")\n",
    "\n",
    "# Create a new VectorStoreIndex using the TimescaleVectorStore\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "### Query the index\n",
    "Now that we've indexed the documents in our VectorStore, we can ask questions about our documents in the index by using the default `query_engine`.\n",
    "\n",
    "Note you can also configure the query engine to configure the top_k most similar results returned, as well as metadata filters to filter the results by. See the [configure standard query setting section](https://gpt-index.readthedocs.io/en/stable/core_modules/data_modules/index/vector_store_guide.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a2bcc07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:29.486725Z",
     "start_time": "2023-06-08T13:06:27.565039Z"
    }
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Did the author work at YC?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cf55bf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:32.686121Z",
     "start_time": "2023-06-08T13:06:32.680098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the author did work at YC.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:41.307336Z",
     "start_time": "2023-06-08T13:06:34.185641Z"
    }
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdf5287f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:44.174719Z",
     "start_time": "2023-06-08T13:06:44.163087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on writing and programming growing up. They wrote short stories and tried writing\n",
      "programs on an IBM 1401 computer. They also built a microcomputer and started programming on it,\n",
      "writing simple games and a word processor.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Querying existing index\n",
    "In the example above, we created a new vectorstore and index from documents we loaded. Next we'll look at how to query an existing index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:07:11.849139Z",
     "start_time": "2023-06-08T13:07:09.040607Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_store = TimescaleVectorStore.from_params(\n",
    "    service_url=TIMESCALE_SERVICE_URL,\n",
    "    table_name=\"paul_graham_essay\",\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:07:19.217191Z",
     "start_time": "2023-06-08T13:07:19.204408Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on writing and programming outside of school. They wrote short stories and tried\n",
      "writing programs on an IBM 1401 computer. They also built a microcomputer and started programming on\n",
      "it, writing simple games and a word processor.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88fcc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
