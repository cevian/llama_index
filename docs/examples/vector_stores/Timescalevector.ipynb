{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db0855d0",
   "metadata": {},
   "source": [
    "# Timescale Vector Store (PostgreSQL)\n",
    "\n",
    "This notebook shows how to use the Postgres vector store `TimescaleVector` to store and query vector embeddings.\n",
    "\n",
    "## What is Timescale Vector?\n",
    "**[Timescale Vector](https://www.timescale.com/ai) is PostgreSQL++ for AI applications.**\n",
    "\n",
    "Timescale Vector enables you to efficiently store and query billions of vector embeddings in `PostgreSQL`.\n",
    "- Enhances `pgvector` with faster and more accurate similarity search on 1B+ vectors via DiskANN inspired indexing algorithm.\n",
    "- Enables fast time-based vector search via automatic time-based partitioning and indexing.\n",
    "- Provides a familiar SQL interface for querying vector embeddings and relational data.\n",
    "\n",
    "Timescale Vector scales with you from POC to production:\n",
    "- Simplifies operations by enabling you to store relational metadata, vector embeddings, and time-series data in a single database.\n",
    "- Benefits from rock-solid PostgreSQL foundation with enterprise-grade feature liked streaming backups and replication, high-availability and row-level security.\n",
    "- Enables a worry-free experience with enterprise-grade security and compliance.\n",
    "\n",
    "## How to use Timescale Vector\n",
    "Timescale Vector is available on [Timescale](https://www.timescale.com/products), the cloud PostgreSQL platform. (There is no self-hosted version at this time.)\n",
    "\n",
    "- LlamaIndex users get a 90-day free trial for Timescale Vector.\n",
    "- To get started, [signup](https://console.cloud.timescale.com/signup) to Timescale, create a new database and follow this notebook!\n",
    "- See the [installation instructions](https://github.com/timescale/python-vector) for more details on using Timescale Vector in python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfb491c6",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "Let's import everything we'll need for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d1c538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:14.461233Z",
     "start_time": "2023-06-08T13:06:14.451565Z"
    }
   },
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import sys\n",
    "\n",
    "# Uncomment to see debug logs\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import SimpleDirectoryReader, StorageContext\n",
    "from llama_index.indices.vector_store import VectorStoreIndex\n",
    "from llama_index.vector_stores import TimescaleVectorStore\n",
    "import textwrap\n",
    "import openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26c71b6d",
   "metadata": {},
   "source": [
    "### Setup OpenAI API Key\n",
    "To create embeddings for documents loaded into the index, let's configure your OpenAI API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b86621",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:16.584215Z",
     "start_time": "2023-06-08T13:06:16.576508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get openAI api key by reading local .env file\n",
    "# The .env file should contain a line starting with `OPENAI_API_KEY=sk-`\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# OR set it explicitly \n",
    "#import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"<your key>\"\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "### Loading documents\n",
    "For this example, we'll use a [SimpleDirectoryReader](https://gpt-index.readthedocs.io/en/stable/examples/data_connectors/simple_directory_reader.html) to load the documents stored in the the `paul_graham_essay` directory. \n",
    "\n",
    "The `SimpleDirectoryReader` is one of LlamaIndex's most commonly used data connectors to read one or multiple files from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c154dd4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:19.009847Z",
     "start_time": "2023-06-08T13:06:19.001593Z"
    }
   },
   "outputs": [],
   "source": [
    "# load sample data from the data directory using a SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"../data/paul_graham\").load_data()\n",
    "print(\"Document ID:\", documents[0].doc_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bd24f0a",
   "metadata": {},
   "source": [
    "### Create a PostgreSQL database and get a Timescale service URL\n",
    "You need a service url to connect to your Timescale database instance.\n",
    "\n",
    "To connect to your cloud PostgreSQL database in Timescale, you'll need your service URI, which can be found in the cheatsheet file you downloaded after creating a new database. The URI will look something like this: `postgres://tsdbadmin:<password>@<id>.tsdb.cloud.timescale.com:<port>/tsdb?sslmode=require`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d61e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the service url by reading local .env file\n",
    "# The .env file should contain a line starting with `TIMESCALE_SERVICE_URL=postgresql://`\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "TIMESCALE_SERVICE_URL = os.environ[\"TIMESCALE_SERVICE_URL\"]\n",
    "\n",
    "# OR set it explicitly\n",
    "# TIMESCALE_SERVICE_URL = \"postgres://tsdbadmin:<password>@<id>.tsdb.cloud.timescale.com:<port>/tsdb?sslmode=require\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0232fd1",
   "metadata": {},
   "source": [
    "## 1. Simple Similarity Search with Timescale Vector\n",
    "\n",
    "### Create a VectorStore Index with the TimescaleVectorStore\n",
    "Next, to perform a similarity search, we first create a `TimescaleVector` [vector store](https://gpt-index.readthedocs.io/en/stable/core_modules/data_modules/storage/vector_stores.html) to store our vector embeddings from the essay content. TimescaleVectorStore takes a few arguments, namely the `service_url` which we loaded above, along with a `table_name` which we will be the name of the table that the vectors are stored in.\n",
    "\n",
    "Then we create a [Vector Store Index](https://gpt-index.readthedocs.io/en/stable/community/integrations/vector_stores.html#vector-store-index) on the documents backed by Timescale using the previously documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8731da62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:23.856002Z",
     "start_time": "2023-06-08T13:06:20.486349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a TimescaleVectorStore to store the documents\n",
    "vector_store = TimescaleVectorStore.from_params(\n",
    "    service_url=TIMESCALE_SERVICE_URL,\n",
    "    table_name=\"paul_graham_essay\",\n",
    ")\n",
    "\n",
    "# Create a new VectorStoreIndex using the TimescaleVectorStore\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "### Query the index\n",
    "Now that we've indexed the documents in our VectorStore, we can ask questions about our documents in the index by using the default `query_engine`.\n",
    "\n",
    "Note you can also configure the query engine to configure the top_k most similar results returned, as well as metadata filters to filter the results by. See the [configure standard query setting section](https://gpt-index.readthedocs.io/en/stable/core_modules/data_modules/index/vector_store_guide.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bcc07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:29.486725Z",
     "start_time": "2023-06-08T13:06:27.565039Z"
    }
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Did the author work at YC?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf55bf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:32.686121Z",
     "start_time": "2023-06-08T13:06:32.680098Z"
    }
   },
   "outputs": [],
   "source": [
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:41.307336Z",
     "start_time": "2023-06-08T13:06:34.185641Z"
    }
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What did the author work on before college?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5287f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:06:44.174719Z",
     "start_time": "2023-06-08T13:06:44.163087Z"
    }
   },
   "outputs": [],
   "source": [
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Querying existing index\n",
    "In the example above, we created a new Timescale Vector vectorstore and index from documents we loaded. Next we'll look at how to query an existing index. All we need is the service URI and the table name we want to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:07:11.849139Z",
     "start_time": "2023-06-08T13:07:09.040607Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_store = TimescaleVectorStore.from_params(\n",
    "    service_url=TIMESCALE_SERVICE_URL,\n",
    "    table_name=\"paul_graham_essay\",\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do before YC?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T13:07:19.217191Z",
     "start_time": "2023-06-08T13:07:19.204408Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b33a8e8b",
   "metadata": {},
   "source": [
    "## 2. Using ANN search indexes to speed up queries\n",
    "\n",
    "(Note: These indexes are ANN indexes, and differ from the index concept in LlamaIndex)\n",
    "\n",
    "You can speed up similarity queries by creating an index on the embedding column. You should only do this once you have ingested a large part of your data.\n",
    "\n",
    "Timescale Vector supports the following indexes:\n",
    "- timescale_vector_index: a disk-ann inspired graph index for fast similarity search (default).\n",
    "- pgvector's HNSW index: a hierarchical navigable small world graph index for fast similarity search.\n",
    "- pgvector's IVFFLAT index: an inverted file index for fast similarity search.\n",
    "\n",
    "Important note: In PostgreSQL, each table can only have one index on a particular column. So if you'd like to test the performance of different index types, you can do so either by (1) creating multiple tables with different indexes, (2) creating multiple vector columns in the same table and creating different indexes on each column, or (3) by dropping and recreating the index on the same column and comparing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the TimescaleVectorStore from part 1\n",
    "vector_store = TimescaleVectorStore.from_params(\n",
    "    service_url=TIMESCALE_SERVICE_URL,\n",
    "    table_name=\"paul_graham_essay\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00a4fbee",
   "metadata": {},
   "source": [
    "Using the `create_index()` function without additional arguments will create a `timescale_vector (DiskANN)` index by default, using the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9e60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timescale vector index (DiskANN)\n",
    "vector_store.create_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6b74702",
   "metadata": {},
   "source": [
    "You can also specify the parameters for the index. See the Timescale Vector documentation for a full discussion of the different parameters and their effects on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f838181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop old index\n",
    "vector_store.drop_index()\n",
    "\n",
    "# create new timescale vector index (DiskANN) with specified parameters\n",
    "vector_store.create_index(\"tsv\", max_alpha=1.0, num_neighbors=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "494826c3",
   "metadata": {},
   "source": [
    "Timescale Vector also supports HNSW and ivfflat indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66606b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.drop_index()\n",
    "\n",
    "# Create an HNSW index\n",
    "# Note: You don't need to specify m and ef_construction parameters as we set smart defaults.\n",
    "vector_store.create_index(\"hnsw\", m=16, ef_construction=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3775ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an IVFFLAT index\n",
    "# Note: You don't need to specify num_lists and num_records parameters as we set smart defaults.\n",
    "vector_store.drop_index()\n",
    "vector_store.create_index(\"ivfflat\", num_lists=20, num_records=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27df7dcd",
   "metadata": {},
   "source": [
    "We recommend using `timescale-vector` or `HNSW` indexes in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the ivfflat index\n",
    "vector_store.drop_index()\n",
    "# Create a timescale vector index (DiskANN)\n",
    "vector_store.create_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9aa5fda2",
   "metadata": {},
   "source": [
    "## 3. Similarity Search with time-based filtering\n",
    "\n",
    "A key use case for Timescale Vector is efficient time-based vector search. Timescale Vector enables this by automatically partitioning vectors (and associated metadata) by time. This allows you to efficiently query vectors by both similarity to a query vector and time.\n",
    "\n",
    "Time-based vector search functionality is helpful for applications like:\n",
    "- Storing and retrieving LLM response history (e.g. chatbots)\n",
    "- Finding the most recent embeddings that are similar to a query vector (e.g recent news).\n",
    "- Constraining similarity search to a relevant time range (e.g asking time-based questions about a knowledge base)\n",
    "\n",
    "To illustrate how to use TimescaleVector's time-based vector search functionality, we'll ask questions about the git log history for TimescaleDB. Each git commit entry has a timestamp associated with it, as well as message and other metadata (e.g author etc). \n",
    "\n",
    "We'll illustrate how to create nodes with a time-based uuid and how run similarity searches with time range filters using the TimescaleVector vectorstore."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5436fd4b",
   "metadata": {},
   "source": [
    "### Extract content and metadata from git log CSV file\n",
    "\n",
    "First lets load in the git log csv file into a new collection in our PostgreSQL database named `timescale_commits`.\n",
    "\n",
    "Note: Since this is a demo, we will only work with the first 1000 records. In practice, you can load as many records as you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe94773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "#file_path = Path('../../../../commit_history.csv')\n",
    "file_path = Path(\"../data/csv/commit_history.csv\")\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Light data cleaning on CSV\n",
    "df.dropna(inplace=True)\n",
    "df = df.astype(str)\n",
    "df = df[:1000]\n",
    "\n",
    "# Display the DataFrame (optional)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c9344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data in the csv (optional)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b9139b5",
   "metadata": {},
   "source": [
    "We'll define a helper funciton to create a uuid for a node and associated vector embedding based on its timestamp. We'll use this function to create a uuid for each git log entry.\n",
    "\n",
    "Important note: If you are working with documents/nodes and want the current date and time associated with vector for time-based search, you can skip this step. A uuid will be automatically generated when the nodes are added to the table in Timescale Vector by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef55d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timescale_vector import client\n",
    "# Function to take in a date string in the past and return a uuid v1\n",
    "def create_uuid(date_string: str):\n",
    "    if date_string is None:\n",
    "        return None\n",
    "    time_format = '%a %b %d %H:%M:%S %Y %z'\n",
    "    datetime_obj = datetime.strptime(date_string, time_format)\n",
    "    uuid = client.uuid_from_time(datetime_obj)\n",
    "    return str(uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d622744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "from typing import List, Tuple\n",
    "# Helper function to split name and email given an author string consisting of Name Lastname <email>\n",
    "def split_name(input_string: str) -> Tuple[str, str]:\n",
    "    if input_string is None:\n",
    "        return None, None\n",
    "    start = input_string.find(\"<\")\n",
    "    end = input_string.find(\">\")\n",
    "    name = input_string[:start].strip()\n",
    "    return name\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "def create_date(input_string: str) -> datetime:\n",
    "    if input_string is None:\n",
    "        return None\n",
    "    # Define a dictionary to map month abbreviations to their numerical equivalents\n",
    "    month_dict = {\n",
    "        \"Jan\": \"01\",\n",
    "        \"Feb\": \"02\",\n",
    "        \"Mar\": \"03\",\n",
    "        \"Apr\": \"04\",\n",
    "        \"May\": \"05\",\n",
    "        \"Jun\": \"06\",\n",
    "        \"Jul\": \"07\",\n",
    "        \"Aug\": \"08\",\n",
    "        \"Sep\": \"09\",\n",
    "        \"Oct\": \"10\",\n",
    "        \"Nov\": \"11\",\n",
    "        \"Dec\": \"12\",\n",
    "    }\n",
    "\n",
    "    # Split the input string into its components\n",
    "    components = input_string.split()\n",
    "    # Extract relevant information\n",
    "    day = components[2]\n",
    "    month = month_dict[components[1]]\n",
    "    year = components[4]\n",
    "    time = components[3]\n",
    "    timezone_offset_minutes = int(components[5])  # Convert the offset to minutes\n",
    "    timezone_hours = timezone_offset_minutes // 60  # Calculate the hours\n",
    "    timezone_minutes = timezone_offset_minutes % 60  # Calculate the remaining minutes\n",
    "    # Create a formatted string for the timestamptz in PostgreSQL format\n",
    "    timestamp_tz_str = f\"{year}-{month}-{day} {time}+{timezone_hours:02}{timezone_minutes:02}\"\n",
    "    return timestamp_tz_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fdaf15f",
   "metadata": {},
   "source": [
    "Next, we'll define a function to create a `TextNode` for each git log entry. We'll use the uuid function we defined above to create a uuid for each node. And we'll use the helper functions above to extract relevant metadata from the git log entry and add them to the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ad668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
    "# Create a Node object from a single row of data\n",
    "def create_node(row):\n",
    "    record = row.to_dict()\n",
    "    record_name = split_name(record[\"author\"])\n",
    "    record_content = str(record[\"date\"]) + \" \" + record_name + \" \" + str(record[\"change summary\"]) + \" \" + str(record[\"change details\"]) \n",
    "    # Can change to TextNode as needed\n",
    "    node = TextNode(\n",
    "        id_=create_uuid(record[\"date\"]),\n",
    "        text= record_content,\n",
    "        metadata={\n",
    "            'commit': record[\"commit\"],\n",
    "            'author': record_name,\n",
    "            'date': create_date(record[\"date\"]),\n",
    "        }\n",
    "    )\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58450993",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [create_node(row) for _, row in df.iterrows()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83c2d55c",
   "metadata": {},
   "source": [
    "Next we'll create vector embeddings of the content of each node so that we can perform similarity search on the the text associated with each node. We'll use the `OpenAIEmbedding` model to create the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa62342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for nodes\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "embedding_model = OpenAIEmbedding()\n",
    "\n",
    "for node in nodes:\n",
    "    node_embedding = embedding_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf1fc37d",
   "metadata": {},
   "source": [
    "Let's examine the first node in our collection to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99fa068",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[0].get_embedding())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47b9f7ee",
   "metadata": {},
   "source": [
    "### Load documents and metadata into TimescaleVector vectorstore\n",
    "Now that we have prepared our nodes and added embeddings to them, let's add them into our TimescaleVector vectorstore.\n",
    "\n",
    "We'll create a Timescale Vector instance from the list of nodes we created.\n",
    "\n",
    "First, we'll define a collection name, which will be the name of our table in the PostgreSQL database. \n",
    "\n",
    "We'll also define a time delta, which we pass to the `time_partition_interval` argument, which will be used to as the interval for partitioning the data by time. Each partition will consist of data for the specified length of time. We'll use 7 days for simplicity, but you can pick whatever value make sense for your use case -- for example if you query recent vectors frequently you might want to use a smaller time delta like 1 day, or if you query vectors over a decade long time period then you might want to use a larger time delta like 6 months or 1 year.\n",
    "\n",
    "Then we'll add the nodes to the Timescale Vector vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db159054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timescale vector store and add the newly created nodes to it\n",
    "ts_vector_store = TimescaleVectorStore.from_params(\n",
    "    service_url=TIMESCALE_SERVICE_URL,\n",
    "    table_name=\"li_commit_history\",\n",
    "    time_partition_interval= timedelta(days=7),\n",
    ")\n",
    "ts_vector_store.add(nodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a44793e3",
   "metadata": {},
   "source": [
    "### Querying vectors by time and similarity\n",
    "\n",
    "Now that we have loaded our documents into TimescaleVector, we can query them by time and similarity.\n",
    "\n",
    "TimescaleVector provides multiple methods for querying vectors by doing similarity search with time-based filtering Let's take a look at each method below.\n",
    "\n",
    "First we define a query string and get the vector embedding for the query string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query and generate embedding for it\n",
    "query_str = \"What's new with TimescaleDB functions?\"\n",
    "embed_model = OpenAIEmbedding()\n",
    "query_embedding = embed_model.get_query_embedding(query_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "317d302e",
   "metadata": {},
   "source": [
    "Then we set some variables which we'll use in our time filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time filter variables for query\n",
    "start_dt = datetime(2023, 8, 1, 22, 10, 35) # Start date = 1 August 2023, 22:10:35\n",
    "end_dt = datetime(2023, 8, 30, 22, 10, 35) # End date = 30 August 2023, 22:10:35\n",
    "td = timedelta(days=7) # Time delta = 7 days"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "374662b5",
   "metadata": {},
   "source": [
    "Method 1: Filter within a provided start date and end date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92bf8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the vector database\n",
    "from llama_index.vector_stores.types import VectorStoreQuery\n",
    "vector_store_query = VectorStoreQuery(query_embedding = query_embedding, similarity_top_k=5)\n",
    "\n",
    "# return most similar vectors to query between start date and end date date range\n",
    "# returns a VectorStoreQueryResult object \n",
    "query_result = ts_vector_store.query(vector_store_query, start_date = start_dt, end_date = end_dt)\n",
    "query_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f6f2825",
   "metadata": {},
   "source": [
    "Let's inspect the nodes that were returned from the similarity search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e8cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each node in the query result, print the node metadata date\n",
    "for node in query_result.nodes:\n",
    "    print(\"-\" * 80)\n",
    "    print(node.metadata[\"date\"])\n",
    "    print(node.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6df2d314",
   "metadata": {},
   "source": [
    "Note how the query only returns results within the specified date range."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8014db61",
   "metadata": {},
   "source": [
    "Method 2: Filter within a provided start date, and a time delta later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33098109",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_query = VectorStoreQuery(query_embedding = query_embedding, similarity_top_k=5)\n",
    "\n",
    "# return most similar vectors to query from start date and a time delta later\n",
    "query_result = ts_vector_store.query(vector_store_query, start_date = start_dt, time_delta = td)\n",
    "\n",
    "for node in query_result.nodes:\n",
    "    print(\"-\" * 80)\n",
    "    print(node.metadata[\"date\"])\n",
    "    print(node.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90b4967a",
   "metadata": {},
   "source": [
    "Once again, notice how only nodes between the start date and the defined time delta later are returned."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "082a4215",
   "metadata": {},
   "source": [
    "Method 3: Filter within a provided end date and a time delta earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a6874",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_query = VectorStoreQuery(query_embedding = query_embedding, similarity_top_k=5)\n",
    "\n",
    "# return most similar vectors to query from end date and a time delta earlier\n",
    "query_result = ts_vector_store.query(vector_store_query, end_date = end_dt, time_delta = td)\n",
    "\n",
    "for node in query_result.nodes:\n",
    "    print(\"-\" * 80)\n",
    "    print(node.metadata[\"date\"])\n",
    "    print(node.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1725d33",
   "metadata": {},
   "source": [
    "The main takeaway is that in each result above, only vectors within the specified time range are returned. These queries are very efficient as they only need to search the relevant partitions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
